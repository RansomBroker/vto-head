<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Face Landmark Detection with MediaPipe and Three.js</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three/examples/js/loaders/GLTFLoader.js"></script>
    <script src="RGBELoader.js"></script>
    <style>
      body {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        margin: 0;
        background-color: #f0f0f0;
      }
      .canvasContainer {
        position: relative;
        width: 640px;
        height: 480px;
      }
      video {
        display: none;
      }
      canvas {
        position: absolute;
        top: 0;
        left: 0;
      }
      #outputCanvas {
        z-index: 1;
      }
      #threeCanvas {
        z-index: 2;
      }
    </style>
  </head>
  <body>
    <video id="videoInput" playsinline></video>
    <div class="canvasContainer">
      <canvas id="outputCanvas" width="640" height="480"></canvas>
      <canvas id="threeCanvas" width="640" height="480"></canvas>
    </div>

    <script>
      const videoElement = document.getElementById("videoInput");
      const outputCanvas = document.getElementById("outputCanvas");
      const outputCtx = outputCanvas.getContext("2d");
      const threeCanvas = document.getElementById("threeCanvas");

      let renderer, scene, camera;
      let glasess;
      const faceLandmarks = [];

      // Function to start the video stream
      async function startVideo() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: "user" },
          });
          videoElement.srcObject = stream;
          await videoElement.play();
        } catch (err) {
          console.error("Error accessing the camera: ", err);
        }
      }

      startVideo();

      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });

      faceMesh.onResults(onResults);

      // Three.js setup
      try {
        renderer = new THREE.WebGLRenderer({
          canvas: threeCanvas,
          alpha: true,
          antialias: true,
        });
        renderer.setSize(threeCanvas.width, threeCanvas.height);
        renderer.autoClear = false;
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(
          80,
          threeCanvas.width / threeCanvas.height,
          0.1,
          1000
        );

        camera.position.z = 500;

        const pmremGenerator = new THREE.PMREMGenerator(renderer);
        pmremGenerator.compileEquirectangularShader();

        new THREE.RGBELoader()
          .setDataType(THREE.HalfFloatType)
          .load("hotel_room_1k.hdr", function (texture) {
            const envMap = pmremGenerator.fromEquirectangular(texture).texture;
            pmremGenerator.dispose();
            scene.environment = envMap;
          });

        // improve WebGLRenderer settings:
        renderer.toneMapping = THREE.ACESFilmicToneMapping;
        renderer.outputEncoding = THREE.sRGBEncoding;
      } catch (e) {
        console.error(
          "WebGL not supported, falling back on experimental-webgl",
          e
        );
        renderer = null;
        alert(
          "WebGL not supported, please update your browser or GPU drivers."
        );
      }

      const loader = new THREE.GLTFLoader();

      loader.load(
        "sunglasses.glb",
        (gltf) => {
          glasess = gltf.scene;
          glasess.traverse((child) => {
            if (child.isMesh) {
              child.renderOrder = 2; // Pastikan gelang dirender setelah occluder
            }
          });

          scene.add(glasess);
          console.log("glasess model loaded successfully");
        },
        undefined,
        (error) => {
          console.error("An error occurred loading the glasess model: ", error);
        }
      );

      // Function to calculate the distance between two landmarks
      function calculateDistance(landmark1, landmark2) {
        const dx = landmark1.x - landmark2.x;
        const dy = landmark1.y - landmark2.y;
        const dz = landmark1.z - landmark2.z;
        return Math.sqrt(dx * dx + dy * dy + dz * dz);
      }

      function onResults(results) {
        outputCtx.save();
        outputCtx.clearRect(0, 0, outputCanvas.width, outputCanvas.height);
        outputCtx.translate(outputCanvas.width, 0);
        outputCtx.scale(-1, 1);
        outputCtx.drawImage(
          results.image,
          0,
          0,
          outputCanvas.width,
          outputCanvas.height
        );

        if (
          results.multiFaceLandmarks &&
          results.multiFaceLandmarks.length > 0
        ) {
          const landmarks = results.multiFaceLandmarks[0];

          if (landmarks) {
            const centerEye = landmarks[168];

            // Position and scale the bracelet
            const glasessSize = calculateDistance(
              landmarks[162],
              landmarks[389]
            );
            const centerEyeX =
              (1 - centerEye.x) * outputCanvas.width - outputCanvas.width / 2;
            const centerEyeY =
              -centerEye.y * outputCanvas.height + outputCanvas.height / 2;
            const centerEyeZ = -centerEye.z * 100;

            glasess.position.set(centerEyeX, centerEyeY, centerEyeZ);
            glasess.scale.set(
              glasessSize * 420,
              glasessSize * 420,
              glasessSize * 420
            );

            // Extract necessary landmarks
            const nose = landmarks[1];
            const chin = landmarks[152];
            const leftEye = landmarks[33];
            const rightEye = landmarks[263];
            const leftEar = landmarks[234];
            const rightEar = landmarks[454];

            // Calculate vectors for face orientation
            const eyeMid = new THREE.Vector3(
              (leftEye.x + rightEye.x) / 2,
              (leftEye.y + rightEye.y) / 2,
              (leftEye.z + rightEye.z) / 2
            );
            const earMid = new THREE.Vector3(
              (leftEar.x + rightEar.x) / 2,
              (leftEar.y + rightEar.y) / 2,
              (leftEar.z + rightEar.z) / 2
            );

            const forward = new THREE.Vector3()
              .subVectors(nose, earMid)
              .normalize();
            const up = new THREE.Vector3().subVectors(chin, eyeMid).normalize();

            const right = new THREE.Vector3()
              .crossVectors(up, forward)
              .normalize();

            // Create rotation matrix
            const rotationMatrix = new THREE.Matrix4();
            rotationMatrix.makeBasis(right, up, forward);

            // Convert to quaternion
            const quaternion = new THREE.Quaternion().setFromRotationMatrix(
              rotationMatrix
            );

            // Correct the quaternion if needed
            quaternion.multiply(
              new THREE.Quaternion().setFromAxisAngle(
                new THREE.Vector3(0, 1, 0),
                Math.PI
              )
            );

            glasess.setRotationFromQuaternion(quaternion);

            glasess.setRotationFromQuaternion(quaternion);
          }
        }

        renderer.render(scene, camera);
        outputCtx.restore();
      }

      const cameraStream = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 640,
        height: 480,
      });

      cameraStream.start();
    </script>
  </body>
</html>
